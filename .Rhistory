a<-grep("case", TrigramData$words)
head(TrigramData$words[a])
tail(TrigramData$words[a])
a<-grep(" case ", TrigramData$words)
head(TrigramData$words[a])
tail(TrigramData$words[a])
a<-grep("^case ", TrigramData$words)
head(TrigramData$words[a])
tail(TrigramData$words[a])
head(str(corpus))
str(corpus[1])
str(corpus[[1]])
table(TrigramData$words[a])
head(TrigramData$frequency[a])
max(TrigramData$frequency[a])
head(order(TrigramData$frequency[a]))
?order
head(order(TrigramData$frequency[a], decreasing = TRUE))
head(TrigramData$word[a]))
head(TrigramData$word[a])
head(TrigramData$words[a])
a<-grep("^would mean  ", TrigramData$words)
head(TrigramData$words(a))
head(TrigramData$words[a])
?tokens
?Token
library(ngram)
?words
?words()
?ngram
str(corpus$dmeta)
corpus$dmeta[1]
corpus$dmeta[1]
corpus$content1[]
corpus$content[1]
corpus$content[[1]]
corpus@content
corpus$content[[1]]
corpus$content$content[1]
corpus$content[1]
str(corpus$content)
str(corpus$content[[1]])
str(corpus$content[[1]]$content)
str(corpus$content[[2]]$content)
str(corpus$content[[3]]$content)
str(corpus$content[[4]]$content)
a<-corpus$content[[4:5]]$content
corpus$content[[4:5]]$content
corpus$content[[4]]$content
corpus$content[[5]]$content
a[1]<-corpus$content[[4]]$content
corpus$content[[6]]$content
corpus$content[[7]]$content
a[2]<-corpus$content[[7]]$content
a
ngram(a, 3)
b<-ngram(a, 3)
b
str(b)
b@n
b@sl_ptr
b@strlen
b@ngsize
b@str_ptr
b@str_ptr
a[b@str_ptr]
str <- "A B A C A B B"
ngram(str, n=2)
str
str <- "A B A C A B B"
ngrams(str, n=2)
?ngrams
str <- "A B A C A B B"
ngrams(str, n=2, sep= " ")
ngram(str, n=2, sep= " ")
install.packages(markovchain)
install.packages(markovchain)
install.packages("markovchain")
install.packages(c("ggplot2", "knitr"))
load("~/Documents/Ranalysis/Capstone/Exploratory/Exploratory.RData")
load("~/Documents/R analysis/Capstone/Exploratory/Exploratory.RData")
basic_info[1]
blog_info[1]
str(blog_info)
blog_info[1]
str(blog_info[[1]])
str(blog_info[1])
a<-(blog_info[[1]])
install.packages("RWeka")
library(ngram)
install.packages("ngram")
library(ngram)
b<-ngram(a,3)
?ngram
str(a)
library(tm)
install.packages(tm)
install.packages("tm")
?tm_map
library(tm)
?tm_map
a
rm(a)
news[[1]]
news[[2]]
a<-news[[2]]
a
str(a)
tm_map(a, PlainTextDocument)
a<-c(news[[2]],news[[3]])
a[1]
a[2]
a
?VCorpus
?VertorSource
?VectorSource
?VCorpus
b<-VectorSource(a)
str(b)
tm_map(b, PlainTextDocument)
?tm_map
Va<-VCorpus(b)
str(Va)
class(a)
class(b)
class(b[[1]])
class(b[1])
class(b[2])
class(b$content)
class(b$encoding)
class(b$reader())
class(b$position)
class(b)
?SimpleSource
ngram_asweka(a)
str(a)
rm(Va)
rm(b)
a[2]
ngram(a[1])
b<-ngram(a[1])
str(b)
summary(b)
str(b)
ngram-print(b)
show(b)
print(b)
c<-print(b)
str(b)
c<-show(b)
get.phrasetable(b)
a[1]
str(a[1])
str(a)
get.ngrams(b)
a<-c("The St. Louis plant had to close", "It would die of old age", " Workers had been making cars there since the onset of mass automotive production in the 1920s")
a
a<-c("the st.Louis plant had to close", "it would die of old age", " workers had been making cars there since the onset of mass automotive production in the 1920s")
ngram(a)
get.ngrams(ngram(a))
class(corpus)
corpus[[1]]
corpus[1]
corpus[[1]]$content
corpus[[2]]$content
corpus[[100]]$content
corpus[[101]]$content
str(data.sample)
data.sample[1]
data.sample[2]
data.sample[]
a<-preprocess(data.sample, case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE)
a<-preprocess(a, case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE)
str(a)
b<-sapply(a,preprocess( ,case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE))
b<-sapply(a,preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE))
b<-sapply(a,function (x) preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE))
b
str(b)
b[1]
b[2]
b[3]
ngram_asweka(b)
ngram_asweka(a)
a
ngram_asweka(b[3])
a[3]
b<-sapply(data.sample,function (x) preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE))
b[1]
b[2]
str(b)
attr(b, "ATT") <- NULL
size(b)
str(b)
attr(b, "names") <- NULL
str(b)
b[1:3]
b[1:6]
b<-sapply(b, function (x) ngram_asweka(x, min=3, max=3))
ngram_asweka(b[1], min=3, max=3))
ngram_asweka(b[1], min=3, max=3)
b[1]
str(b)
str(a)
b<-sapply(a, function (x) ngram_asweka(x, min=3, max=3))
b
str(a)
a<-sapply(data.sample,function (x) preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = FALSE,  fix.spacing = TRUE))
attr(a, "names") <- NULL
str(a)
b<-sapply(a, function (x) ngram_asweka(x, min=3, max=3))
grep("",a)
grep(" ",a)
?grep
grep("", a, value = TRUE)
head(grep("", a, value = TRUE))
head(grep(" ", a, value = TRUE))
head(sub(" ", a, value = TRUE))
head(sub(" ", a))
b<-sapply(a[1:100], function (x) ngram_asweka(x, min=3, max=3))
b<-sapply(a, function (x) ngram_asweka(x, min=3, max=3))
b<-sapply(a[1:10000], function (x) ngram_asweka(x, min=3, max=3))
b<-sapply(a[1:1000], function (x) ngram_asweka(x, min=3, max=3))
count(a[1])
wordcount(a[1])
b<-sapply(a, wordcount)
b[1]
str(b)
attr(b, "names") <- NULL
str(b)
hist(b)
b[which(b==0)]
b[which(b==1)]
b[which(b==0)]
b[which(b==2)]
b<-sapply(a, function (x) ngram_asweka(x, min=3, max=3))
b<-sapply(a, function (x) ngram_asweka(x, min=2, max=2))
a=2
clear
exit()
q()
?Arima
library(forecast)
?arima
?auto.arima
library(forecast)
?`forecast-package`
c("169.0", "195.0", "164.0", "300.0", "350.0", "325.0", "209.0", "204.0", "143.0",     214.0
11     297.0
12     432.0
13     305.0
14     225.0
15     192.0
16     285.0
17     325.0
18     418.0
19     579.0
20     379.0
21     318.0
22     371.0
23     433.0
24     505.0
25     620.0
26     743.0
27     662.0
28    1966.0
29     776.0
c("169.0", "195.0", "164.0", "300.0", "350.0", "325.0", "209.0", "204.0", "143.0", "214.0"
,"297.0"
,"432.0"
,"305.0"
,"225.0"
,"192.0"
,"285.0"
,"325.0"
,"418.0"
,"579.0"
,"379.0"
,"318.0"
,"371.0"
,"433.0"
,"505.0"
,"620.0"
,"743.0"
,"662.0"
,"1966.0"
,"776.0")
a<- c("169.0", "195.0", "164.0", "300.0", "350.0", "325.0", "209.0", "204.0", "143.0", "214.0"
,"297.0"
,"432.0"
,"305.0"
,"225.0"
,"192.0"
,"285.0"
,"325.0"
,"418.0"
,"579.0"
,"379.0"
,"318.0"
,"371.0"
,"433.0"
,"505.0"
,"620.0"
,"743.0"
,"662.0"
,"1966.0"
,"776.0")
var(a)
a<- c("153", "169.0", "195.0", "164.0", "300.0", "350.0", "325.0", "209.0", "204.0", "143.0", "214.0"
,"297.0"
,"432.0"
,"305.0"
,"225.0"
,"192.0"
,"285.0"
,"325.0"
,"418.0"
,"579.0"
,"379.0"
,"318.0"
,"371.0"
,"433.0"
,"505.0"
,"620.0"
,"743.0"
,"662.0"
,"1966.0"
,"776.0")
var(a)
?var
var(a)
0.4/0.8
log(1)
log(2)
log(3)
log(4)
log(1:10)
0.8/2
L<-0.44
C<-0.3
num<--L-ln(1-C)
?ln
?ln()
?log
num<--L-log(1-C)
den<-log(C)-log(1-C)
num/den
L<-0.44
C<-0.3
num/den
den<-log(C)-log(1-C)
num<--L-log(1-C)
num/den
N=10000
L=1.01
num<--L-log(1-C)
den<-log(C)-log(1-C)
num/den
load("~/Documents/R analysis/Capstone/Model/NgramData.RData")
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
return(Bigram)
}
example<- c("he started telling me about his")
library(timeDate)
library(tm)
a<- prepro(example)
a
str(a)
Freq(a)
q<-grep(paste0("^", "started telling"), TrigramData)
q<-grep(paste0("^", "start telling"), TrigramData)
paste0("^", "start telling")
?grep
head(TrigramData)
q<-grep(paste0("^", "start telling"), TrigramData$words)
q<-grep(paste0("^", "started telling"), TrigramData$words)
q<-grep(paste0("^", "started telling"), TrigramData$words, value= TRUE)
q<-grep(paste0("^", "start telling"), TrigramData$words, value= TRUE)
q
q<-grep(paste0("^", "start tell"), TrigramData$words, value= TRUE)
prepro("I like")
str(prepro("I like"))
Freq(prepro("I like"))
corpus <- VCorpus(VectorSource("I Like"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
corpus
str(corpus)
corpus$`character(0)`
corpus$content
View(Freq)
View(Freq)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
str(Bigram)
Bigram$dimnames$Terms
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (Bigram$dimnames$Terms = NULL){
Onegram<-TermDocumentMatrix(corpus)
return (Onegram)
}
return(Bigram)
}
Bigram$dimnames$Terms = NULL
Bigram$dimnames$Terms == NULL
str(Bigram$dimnames$Terms == NULL)
(Bigram$dimnames$Terms == NULL)
(is.null(Bigram$dimnames$Terms)
)
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (is.null(Bigram$dimnames$Terms = NULL)){
Onegram<-TermDocumentMatrix(corpus)
return (Onegram)
}
return(Bigram)
}
is.null()
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Ngram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if is.null(Ngram$dimnames$Terms = NULL)
Ngram<-TermDocumentMatrix(corpus)
return(Ngram)
}
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Ngram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (is.null(Ngram$dimnames$Terms = NULL))
Ngram<-TermDocumentMatrix(corpus)
return(Ngram)
}
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Ngram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (is.null(Ngram$dimnames$Terms))
Ngram<-TermDocumentMatrix(corpus)
return(Ngram)
}
prepro("I like")
prepro("I like")
b<-prepro("I like")
b
summary(b)
b$dimnames$Terms
words(b$dimnames$Terms)
count(b$dimnames$Terms)
Freq(b)
example="When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd"
example
b<-prepro(example)
Freq(b)
shiny::runApp('Documents/R analysis/Capstone/App/WordPredictor')
publish(user="Pedgalso", repo="git@github.com:Pedgalso/NaturalLanguageModel.git")
library(devtools)
publish(user="Pedgalso", repo="git@github.com:Pedgalso/NaturalLanguageModel.git")
?publish
slidify('index.Rmd')
library(slidify)
slidify('index.Rmd')
setwd("~/Documents/R analysis/Capstone/Model/Pitch")
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
?publish
publish(user = "Pedgalso", repo="NaturalLanguageModel")
publish(user = "Pedgalso", repo="NaturalLanguageModel")
publish(user = "Pedgalso", repo="NaturalLanguageModel")
