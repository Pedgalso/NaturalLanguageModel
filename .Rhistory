corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (is.null(Bigram$dimnames$Terms = NULL)){
Onegram<-TermDocumentMatrix(corpus)
return (Onegram)
}
return(Bigram)
}
is.null()
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Ngram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if is.null(Ngram$dimnames$Terms = NULL)
Ngram<-TermDocumentMatrix(corpus)
return(Ngram)
}
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Ngram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (is.null(Ngram$dimnames$Terms = NULL))
Ngram<-TermDocumentMatrix(corpus)
return(Ngram)
}
prepro<- function (x){
corpus <- VCorpus(VectorSource(x))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, PlainTextDocument)
Ngram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
if (is.null(Ngram$dimnames$Terms))
Ngram<-TermDocumentMatrix(corpus)
return(Ngram)
}
prepro("I like")
prepro("I like")
b<-prepro("I like")
b
summary(b)
b$dimnames$Terms
words(b$dimnames$Terms)
count(b$dimnames$Terms)
Freq(b)
example="When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd"
example
b<-prepro(example)
Freq(b)
setwd("~/Documents/R analysis/Capstone/Model")
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.twitter.txt", "r")
twitter<-readLines(con, skipNul = TRUE)
close(con)
a<-sample(twitter, length(news) * 0.03
a<-sample(twitter, length(news) * 0.03)
a<-sample(twitter, length(twitter) * 0.03)
rm(twitter)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.news.txt", "r")
news<-readLines(con, skipNul = TRUE)
close(con)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.blogs.txt", "r")
blog<-readLines(con, skipNul = TRUE)
close(con)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.twitter.txt", "r")
twitter<-readLines(con, skipNul = TRUE)
close(con)
set.seed(280520)
data.sample <- c(sample(twitter, length(twitter) * 0.03),
sample(news, length(news) * 0.03),
sample(blog, length(blog) * 0.03))
rm(a)
rm(blog)
rm(news)
rm(twitter)
rm(con)
NLPBigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "),
use.names = FALSE)
}
NLPTrigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "),
use.names = FALSE)
}
Freq <- function(x){
a<-data.frame(x$i,x$j,x$v)
a<-aggregate(x$v, by=list(x$i), sum)
colnames(a)<-c("wordIndex", "frequency")
a<-a[order(a$frequency, decreasing = TRUE),]
a$words<-x$dimnames$Terms[a$wordIndex]
a<-a[,2:3]
return(a)
}
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = FALSE)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
x <-iconv(x, "latin1", "ASCII")
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
library(tm)
corpus <- VCorpus(VectorSource(data.sample))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(removeMostPunctuation))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
Trigram<-TermDocumentMatrix(corpus, control = list(tokenize=NLPTrigramTokenizer))
BigramData <-Freq(Bigram)
object.size(BigramData)
head(BigramData)
tail(BigramData)
TrigramData<-Freq(Trigram)
head(TrigramData)
tail(TrigramData)
object.size(TrigramData)
object.size(data.sample
)
summary(TrigramData$frequency)
summary(BigramData$frequency)
data.sample2<-sample(data.sample, dim(data.sample)[1]*0.6)
dim(data.sample)[1]
dim(data.sample)
length(data.sample)
data.sample2<-sample(data.sample, length(data.sample)*0.6)
object.size(data.sample2)
corpus <- VCorpus(VectorSource(data.sample2))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(removeMostPunctuation))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
Trigram<-TermDocumentMatrix(corpus, control = list(tokenize=NLPTrigramTokenizer))
TrigramData2<-Freq(Trigram)
object.size(TrigramData2)
head(TrigramData2)
head(TrigramData)
(TrigramData[1:10,]
TrigramData[1:10,]
TrigramData2[1:10,]
rm(Bigram)
rm(corpus)
rm(Trigram)
rm(data.sample)
rm(data.sample2)
rm(TrigramData2)
rm(BigramData)
rm(TrigramData)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.twitter.txt", "r")
twitter<-readLines(con, skipNul = TRUE)
close(con)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.news.txt", "r")
news<-readLines(con, skipNul = TRUE)
close(con)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.blogs.txt", "r")
blog<-readLines(con, skipNul = TRUE)
close(con)
set.seed(280520)
data.sample <- c(sample(twitter, length(twitter) * 0.02),
data.sample <- c(sample(twitter, length(twitter) * 0.02),
sample(news, length(news) * 0.02),
sample(blog, length(blog) * 0.02))
set.seed(280520)
data.sample <- c(sample(twitter, length(twitter) * 0.01),
sample(news, length(news) * 0.02),
sample(blog, length(blog) * 0.02))
set.seed(280520)
data.sample <- c(sample(twitter, length(twitter) * 0.015),
sample(news, length(news) * 0.025),
sample(blog, length(blog) * 0.025))
rm(blog)
rm(news)
rm(twitter)
rm(con)
library(tm)
corpus <- VCorpus(VectorSource(data.sample))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(removeMostPunctuation))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
Trigram<-TermDocumentMatrix(corpus, control = list(tokenize=NLPTrigramTokenizer))
BigramData <-Freq(Bigram)
TrigramData<-Freq(Trigram)
object.size(TrigramData)
object.size(BigramData)
head(TrigramData)
set.seed(280520)
rownames(TrigramData)<-NULL
object.size(TrigramData)
rownames(BigramData)<-NULL
object.size(BigramData)
NLPBigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "),
use.names = FALSE)
}
NLPTrigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "),
use.names = FALSE)
}
Freq <- function(x){
a<-data.frame(x$i,x$j,x$v)
a<-aggregate(x$v, by=list(x$i), sum)
colnames(a)<-c("wordIndex", "frequency")
a<-a[order(a$frequency, decreasing = TRUE),]
a$words<-x$dimnames$Terms[a$wordIndex]
a<-a[,2:3]
return(a)
}
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = FALSE)
{
rmpunct <- function(x) {
x <- gsub("#", "\002", x)
x <- gsub("[[:punct:]]+", "", x)
x <-iconv(x, "latin1", "ASCII")
gsub("\002", "#", x, fixed = TRUE)
}
if (preserve_intra_word_dashes) {
x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
x <- rmpunct(x)
gsub("\001", "-", x, fixed = TRUE)
} else {
rmpunct(x)
}
}
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.twitter.txt", "r")
twitter<-readLines(con, skipNul = TRUE)
close(con)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.news.txt", "r")
news<-readLines(con, skipNul = TRUE)
close(con)
con <- file("~/Documents/R analysis/Capstone/Coursera-SwiftKey/en_US/en_US.blogs.txt", "r")
blog<-readLines(con, skipNul = TRUE)
close(con)
set.seed(280520)
data.sample <- c(sample(twitter, length(twitter) * 0.01),
sample(news, length(news) * 0.02),
sample(blog, length(blog) * 0.02))
library(tm)
corpus <- VCorpus(VectorSource(data.sample))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(removeMostPunctuation))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
Bigram<-TermDocumentMatrix(corpus, control=list(tokenize=NLPBigramTokenizer))
BigramData <-Freq(Bigram)
rownames(BigramData)<-NULL
object.size(BigramData)
Trigram<-TermDocumentMatrix(corpus, control = list(tokenize=NLPTrigramTokenizer))
TrigramData<-Freq(Trigram)
rownames(TrigramData)<-NULL
object.size(TrigramData)
head(TrigramData)
rm(Bigram)
rm(corpus)
rm(Trigram)
rm(blog)
rm(news)
rm(twitter)
rm(con)
rm(data.sample)
rm(Freq)
rm(NLPBigramTokenizer())
rm(NLPBigramTokenizer)
rm(NLPTrigramTokenizer())
rm(NLPTrigramTokenizer)
rm(removeMostPunctuation())
rm(removeMostPunctuation)
save.image("~/Documents/R analysis/Capstone/Model/NgramData.RData")
q2<-"You're the reason why I smile everyday. Can you follow me please? It would mean the"
library(ngram)
a<-sapply(q2, function (x) preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = TRUE,  fix.spacing = TRUE))
attr(a, "names") <- NULL
a
prepro<- function (x){
# perform removepunctuation, removenumbers, tolower, stripWhitespace
a<-sapply(x, function (x) preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = TRUE,  fix.spacing = TRUE))
attr(a, "names") <- NULL
# remove stopwords
#  stopWords <- stopwords("en")
#a <- ngram_asweka(a,1,1)
#a <-a[!(a %in% stopWords)]
#b<-c()
#for (i in a[-1]) b<- paste(b, i)
#a<- paste0(a[1],b)
# perform bigram or onegram
Ngram<- ngram_asweka(a, min=2, max=2)
if (length(Ngram)==0)
Ngram<- ngram_asweka(a, min=1, max=1)
return(Ngram)}
model <- function (x, Tgram, Bgram, gamma2= 0.6, gamma3=0.4) {
lastword <- function(x1){
a <- ngram_asweka(x1, min=1, max=1)
a <- a[length(a)]
return(a)}
# Last Ngram of the user input data
df<-x[length(x)]
# look up in trigram database for matches
s3<- data.frame()
if (length(ngram_asweka(df, min=1, max=1))>1){
s3<-Tgram[grep(paste0("^", df, " "), Tgram$words),]
# keep only top10 matches
if (dim(s3)[1] >10)
s3<-s3[1:10, ]
p3 <- (s3$frequency/ sum(s3$frequency))*gamma3
s3$probability <- p3
}
# look up in bigram database for matches
lw<-lastword(df)
s2<-Bgram[grep(paste0("^", lw, " "), Bgram$words),]
# keep only top10
if (dim(s2)[1] >10)
s2<-s2[1:10, ]
p2 <- (s2$frequency/ sum(s2$frequency))*gamma2
s2$probability <- p2
# Join together bigram and trigram matches
if (nrow(s3)>0)
s<-rbind(s3,s2)
else
s<-s2
# take last word from each bigram and trigram, these are the predicted words
s$words<-sapply(s$words, lastword)
# drop frequency column
s$frequency <- NULL
# join together predicted words from trigrams&bigrams
s<-aggregate(s$probability, by=list(s$word), sum)
s<-s[order(s$x, decreasing=TRUE),]
return(s)}
a<-prepro(q2)
a
model(a, TrigramData, BigramData, gamma2= 0.6, gamma3=0.4)
model(a, TrigramData, BigramData, gamma2= 0.5, gamma3=0.5)
q1<-"The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
q1 <-prepro(q1)
q1 <-model(q1, TrigramData, BigramData, gamma2= 0.6, gamma3=0.4)
head(q1)
q2<-"You're the reason why I smile everyday. Can you follow me please? It would mean the"
q2 <-prepro(q2)
q2 <-model(q2, TrigramData, BigramData, gamma2= 0.6, gamma3=0.4)
head(q2)
q3<-"Hey sunshine, can you follow me and make me the"
q3 <-prepro(q3)
q3 <-model(q3, TrigramData, BigramData, gamma2= 0.6, gamma3=0.4)
head(q3)
q4<-"Very early observations on the Bills game: Offense still struggling but the"
q4 <-prepro(q4)
q4<- model(q4, TrigramData, BigramData, gamma2= 0.6, gamma3=0.4)
head(q4)
shiny::runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
rnorm(10)
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
setwd("~/Documents/R analysis/Capstone/Model")
load("~/Documents/R analysis/Capstone/Model/NgramData.RData")
prepro<- function (x){
# perform removepunctuation, removenumbers, tolower, stripWhitespace
a<-sapply(x, function (x) preprocess(x ,case="lower", remove.punct = TRUE, remove.numbers = TRUE,  fix.spacing = TRUE))
attr(a, "names") <- NULL
# remove stopwords
#  stopWords <- stopwords("en")
#a <- ngram_asweka(a,1,1)
#a <-a[!(a %in% stopWords)]
#b<-c()
#for (i in a[-1]) b<- paste(b, i)
#a<- paste0(a[1],b)
# perform bigram or onegram
Ngram<- ngram_asweka(a, min=2, max=2)
if (length(Ngram)==0)
Ngram<- ngram_asweka(a, min=1, max=1)
return(Ngram)}
model <- function (x, Tgram, Bgram, gamma2= 0.6, gamma3=0.4) {
lastword <- function(x1){
a <- ngram_asweka(x1, min=1, max=1)
a <- a[length(a)]
return(a)}
# Last Ngram of the user input data
df<-x[length(x)]
# look up in trigram database for matches
s3<- data.frame()
if (length(ngram_asweka(df, min=1, max=1))>1){
s3<-Tgram[grep(paste0("^", df, " "), Tgram$words),]
# keep only top10 matches
if (dim(s3)[1] >10)
s3<-s3[1:10, ]
p3 <- (s3$frequency/ sum(s3$frequency))*gamma3
s3$probability <- p3
}
# look up in bigram database for matches
lw<-lastword(df)
s2<-Bgram[grep(paste0("^", lw, " "), Bgram$words),]
# keep only top10
if (dim(s2)[1] >10)
s2<-s2[1:10, ]
p2 <- (s2$frequency/ sum(s2$frequency))*gamma2
s2$probability <- p2
# Join together bigram and trigram matches
if (nrow(s3)>0)
s<-rbind(s3,s2)
else
s<-s2
# take last word from each bigram and trigram, these are the predicted words
s$words<-sapply(s$words, lastword)
# drop frequency column
s$frequency <- NULL
# join together predicted words from trigrams&bigrams
s<-aggregate(s$probability, by=list(s$word), sum)
s<-s[order(s$x, decreasing=TRUE),]
return(s)}
save.image("~/Documents/R analysis/Capstone/Model/NgramData.RData")
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
?aggregate
model <- function (x, Tgram, Bgram, gamma2= 0.6, gamma3=0.4) {
lastword <- function(x1){
a <- ngram_asweka(x1, min=1, max=1)
a <- a[length(a)]
return(a)}
# Last Ngram of the user input data
df<-x[length(x)]
# look up in trigram database for matches
s3<- data.frame()
if (length(ngram_asweka(df, min=1, max=1))>1){
s3<-Tgram[grep(paste0("^", df, " "), Tgram$words),]
# keep only top10 matches
if (dim(s3)[1] >10)
s3<-s3[1:10, ]
p3 <- (s3$frequency/ sum(s3$frequency))*gamma3
s3$probability <- p3
}
# look up in bigram database for matches
lw<-lastword(df)
s2<-Bgram[grep(paste0("^", lw, " "), Bgram$words),]
# keep only top10
if (dim(s2)[1] >10)
s2<-s2[1:10, ]
p2 <- (s2$frequency/ sum(s2$frequency))*gamma2
s2$probability <- p2
# Join together bigram and trigram matches
if (nrow(s3)>0)
s<-rbind(s3,s2)
else
s<-s2
# take last word from each bigram and trigram, these are the predicted words
s$words<-sapply(s$words, lastword)
# drop frequency column
s$frequency <- NULL
# join together predicted words from trigrams&bigrams
s<-aggregate(s$probability, by=list(s$word), sum)
s<-s[order(s$x, decreasing=TRUE),]
names(s)<-c("Words", "Probability")
return(s)}
save.image("~/Documents/R analysis/Capstone/Model/NgramData.RData")
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
runApp('~/Documents/R analysis/Capstone/App/WordPredictor')
install.packages("leaflet")
install_github('slidify', 'ramnathv')
install.packages("githubinstall")
install_github('slidify', 'ramnathv')
library(githubinstall)
install_github('slidify', 'ramnathv')
library(devtools)
install_github('slidify', 'ramnathv')
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
library(slidify)
author('Pedro')
library(tm)
corpus <- VCorpus(VectorSource(data.sample))
library(tm)
corpus <- VCorpus(VectorSource(data.sample))
